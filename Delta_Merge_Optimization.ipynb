{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599aa62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Delta merge optimization excercise\n",
    "# https://stackoverflow.com/questions/77051683/performance-issue-while-merging-data-to-a-delta-lake\n",
    "\n",
    "from delta.tables import *\n",
    "\n",
    "sdf = spark.range(1000).withColumn(\"id\", (rand() * 30000000 * 2).cast(IntegerType()))\\\n",
    ".withColumn(\"par\", col(\"id\")%5)\\\n",
    ".withColumn(\"ts\", current_timestamp())\n",
    "display(sdf)\n",
    "\n",
    "conditions_list=[\"existing.id = updates.id\"]\n",
    "delta_table_path=\"/random/source/\"\n",
    "\n",
    "if (DeltaTable.isDeltaTable(spark, delta_table_path)):\n",
    "    delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "    delta_table.alias(\"existing\").merge(\n",
    "    source = sdf.alias(\"updates\"),\n",
    "    condition = \" AND \".join(conditions_list)\n",
    "    ).whenMatchedUpdateAll(\n",
    "    ).whenNotMatchedInsertAll(\n",
    "    ).execute()\n",
    "else:\n",
    "    df.write\\\n",
    "        .format(\"delta\")\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .partitionBy(\"par\")\\\n",
    "        .save(\"/random/source/\")\n",
    "\n",
    "# Now add partition condition like below inside your conditions_list to improve performance.\n",
    "update_cols = [i[0] for i in sdf.select(\"par\").distinct().sort(\"par\").collect()]\n",
    "par_val = ','.join(map(str,update_cols))\n",
    "conditions_list=[\"existing.id = updates.id\"]\n",
    "conditions_list.append(f\"existing.par in ({par_val})\")\n",
    "conditions_list\n",
    "\n",
    "#Before running if i check the record give no results.\n",
    "sdf.head(5)\n",
    "\n",
    "tmp= DeltaTable.forPath(spark, delta_table_path).toDF()\n",
    "display(tmp.filter(col(\"par\")==4).filter(col(\"id\")==11942589)) #this number is read from sdf.head(5)\n",
    "\n",
    "#After running the merge if i check the record again it gives results.\n",
    "tmp= DeltaTable.forPath(spark, delta_table_path).toDF()\n",
    "display(tmp.filter(col(\"par\")==4).filter(col(\"id\")==11942589)) #this number is read from sdf.head(5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
